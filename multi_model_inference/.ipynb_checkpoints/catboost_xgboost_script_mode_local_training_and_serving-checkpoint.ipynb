{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost XGBoost Script Mode Training and Serving \n",
    "\n",
    "This is a sample Python program that trains a simple CatBoost model and a XGBoost model using SageMaker XGBoost Docker image, and then performs inference. This implementation will work on your *local computer* or in the *AWS Cloud*.\n",
    "\n",
    "#### Prerequisites:\n",
    "1. Install required Python packages:\n",
    "   `pip install -r requirements.txt`\n",
    "2. Docker Desktop installed and running on your computer:\n",
    "   `docker ps`\n",
    "3. You should have AWS credentials configured on your local machine in order to be able to pull the docker image from ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "    \n",
    "prefix = \"xgboost_catboost\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "Download training and eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train = './data/train/boston_train.csv'\n",
    "local_validation = './data/validation/boston_validation.csv'\n",
    "local_test = './data/test/boston_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/train/boston_train.csv') and \\\n",
    "        os.path.isfile('./data/validation/boston_validation.csv') and \\\n",
    "        os.path.isfile('./data/test/boston_test.csv'):\n",
    "    print('Training dataset exist. Skipping Download')\n",
    "else:\n",
    "    print('Downloading training dataset')\n",
    "\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    os.makedirs(\"./data/train\", exist_ok=True)\n",
    "    os.makedirs(\"./data/validation\", exist_ok=True)\n",
    "    os.makedirs(\"./data/test\", exist_ok=True)\n",
    "\n",
    "    data = load_boston()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=45)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=45)\n",
    "\n",
    "    trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "    trainX['target'] = y_train\n",
    "\n",
    "    valX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "    valX['target'] = y_test\n",
    "\n",
    "    testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "\n",
    "    trainX.to_csv(local_train, header=None, index=False)\n",
    "    valX.to_csv(local_validation, header=None, index=False)\n",
    "    testX.to_csv(local_test, header=None, index=False)\n",
    "\n",
    "    print('Downloading completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Starting model training using **local mode**. Note: if launching for the first time in local mode, container image download might take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_instance_type = \"ml.m5.xlarge\"\n",
    "train_location = sess.upload_data(\n",
    "    local_train, key_prefix=\"{}/data/{}\".format(prefix, \"train\")\n",
    ")\n",
    "validation_location = sess.upload_data(\n",
    "    local_validation, key_prefix=\"{}/data/{}\".format(prefix, \"validation\")\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"num_round\": 6}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"multi_model_deploy.py\",\n",
    "    \"source_dir\": \"code\",\n",
    "    \"dependencies\": [\"my_custom_library\"],\n",
    "    \"instance_type\": training_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"xgboost-model\",\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "}    \n",
    "    \n",
    "\n",
    "estimator = XGBoost(**estimator_parameters)\n",
    "\n",
    "estimator.fit({'train': train_location, 'validation': validation_location})\n",
    "print('Completed model training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying trained model \n",
    "We can also deploy the trained model and perform invocation \n",
    "\n",
    "uncomment the below cell if you would like to deploy directly from the estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"xgboost-catboost-endpoint\"\n",
    "# predictor = estimator.deploy(\n",
    "#         initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a model trained previously, you can use the model s3 uri in the model_data field and create a model object for deployment. No need to retrain the model using the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "\n",
    "inference_model = XGBoostModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point=\"multi_model_deploy.py\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    dependencies=[\"my_custom_library\"],\n",
    "    source_dir=\"code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point script \"multi_model_deploy.py\" will handle the multiple models in the model artifacts and perform inference against each model. The results will be the mean of each inference output. This is a simple demonstration of how to work with multiple models, but you can design the model ensemble as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = inference_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.serializers import NumpySerializer, JSONSerializer, CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer, JSONDeserializer\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_test, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "predictions = predictor.predict(payload)\n",
    "print('predictions: {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear up resources\n",
    "Delete the endpoint deployed in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
