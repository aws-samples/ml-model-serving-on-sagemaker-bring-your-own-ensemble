{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151ddaba",
   "metadata": {},
   "source": [
    "## CatBoost Scikit Learn Script Mode Local Training and Serving \n",
    "\n",
    "This is a sample Python program that trains a simple CatBoost model using SageMaker scikit-learn Docker image, and then performs inference. This implementation will work on your *local computer* or in the *AWS Cloud*.\n",
    "\n",
    "#### Prerequisites:\n",
    "1. Install required Python packages:\n",
    "   `pip install -r requirements.txt`\n",
    "2. Docker Desktop installed and running on your computer:\n",
    "   `docker ps`\n",
    "3. You should have AWS credentials configured on your local machine in order to be able to pull the docker image from ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6020bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print(\"Setting role and SageMaker session manually...\")\n",
    "    \n",
    "    #please change the bucket, region and iam role as needed\n",
    "    region = \"us-west-2\"\n",
    "    bucket = f\"sagemaker-{region}-demo\"\n",
    "    \n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    sagemaker_execution_role_name = (\n",
    "        \"AmazonSageMaker-ExecutionRole-20200101T000001\"  # Change this to your role name\n",
    "    )\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    boto3.setup_default_session(region_name=region, profile_name=\"default\")\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "    \n",
    "prefix = \"catboost_scikit_learn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bb2a7",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "Download training and eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train = './data/train/boston_train.csv'\n",
    "local_validation = './data/validation/boston_validation.csv'\n",
    "local_test = './data/test/boston_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1349eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./data/train/boston_train.csv') and \\\n",
    "        os.path.isfile('./data/validation/boston_validation.csv') and \\\n",
    "        os.path.isfile('./data/test/boston_test.csv'):\n",
    "    print('Training dataset exist. Skipping Download')\n",
    "else:\n",
    "    print('Downloading training dataset')\n",
    "\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    os.makedirs(\"./data/train\", exist_ok=True)\n",
    "    os.makedirs(\"./data/validation\", exist_ok=True)\n",
    "    os.makedirs(\"./data/test\", exist_ok=True)\n",
    "\n",
    "    data = load_boston()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=45)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=45)\n",
    "\n",
    "    trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "    trainX['target'] = y_train\n",
    "\n",
    "    valX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "    valX['target'] = y_test\n",
    "\n",
    "    testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "\n",
    "    trainX.to_csv(local_train, header=None, index=False)\n",
    "    valX.to_csv(local_validation, header=None, index=False)\n",
    "    testX.to_csv(local_test, header=None, index=False)\n",
    "\n",
    "    print('Downloading completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71aa4c3",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Starting model training using **local mode**. Note: if launching for the first time in local mode, container image download might take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change the below parameter to change between \"local\" and \"remote\" training mode \n",
    "#(change to False will launch the training job using a remote training instance)\n",
    "local_training = False\n",
    "if local_training:\n",
    "    training_instance_type = \"local\"\n",
    "    train_location = 'file://' + local_train\n",
    "    validation_location = 'file://' + local_validation\n",
    "else:\n",
    "    training_instance_type = \"ml.m5.xlarge\"\n",
    "    train_location = sess.upload_data(\n",
    "        local_train, key_prefix=\"{}/data/{}\".format(prefix, \"train\")\n",
    "    )\n",
    "    validation_location = sess.upload_data(\n",
    "        local_validation, key_prefix=\"{}/data/{}\".format(prefix, \"validation\")\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SKLearn estimator and call fit() to start the training\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=\"catboost_train_deploy.py\",\n",
    "    source_dir='code',\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=training_instance_type,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "sklearn.fit({'train': train_location, 'validation': validation_location})\n",
    "print('Completed model training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba98f86",
   "metadata": {},
   "source": [
    "## local training behind the scene\n",
    "\n",
    "When user run the training/hosting in \"*local*\" mode, the job will firstly pull the requested docker image from ECR (in this case, we are pulling from the service team account to get the prebuilt [docker container for scikit learn](https://github.com/aws/sagemaker-scikit-learn-container)). You can run the below command in terminal to check which docker images are available on your local instance/machine:\n",
    "```\n",
    "$ docker images\n",
    "```\n",
    "\n",
    "![](./image/docker_images.png)\n",
    "\n",
    "Then the pulled image will be run in the local environment similarly like it would be run in the remote SageMaker training instance. You can mimic the same behavor and run the script in a running docker container manually to understand better how the entry point script is executed inside the running container. \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning:</b> when you run the below command, make sure you are under the current directory (catboost_scikit_learn_script_mode_local_training_and_serving). Also you need to create a \"model\" folder under the current directory, otherwise you will see error message in the model saving stage.\n",
    "</div>\n",
    "\n",
    "![](./image/folder_structure.png)\n",
    "\n",
    "#### <span style=\"color:blue\">Step 1: Running docker image from terminal<span>\n",
    "```\n",
    "$ docker run -v $(pwd):/opt/ml -v $(pwd)/data:/opt/ml/input/data -it $(docker images -f \"reference=*/*0.23-1-cpu-py3\" --quiet)\n",
    "```\n",
    "\n",
    "#### <span style=\"color:blue\">Step 2: Prepare the environment and install python packages<span>\n",
    "\n",
    "Once get inside the running docker container, we can execute the same command that we saw from the logs emitted from the previous local traning job. Firstly, we will install the packages specified in the *requirements.txt* file in the code folder, and then execute the entry point script defined by the job. \n",
    "\n",
    "\n",
    "```\n",
    "# export SM_OUTPUT_DATA_DIR=/opt/ml/model\n",
    "# export SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
    "# export SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
    "# cd /opt/ml/code\n",
    "# python -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "![](./image/docker_run.png)\n",
    "\n",
    "*Note that* the first three command is to set up environment variables manually, but when you run in local or remote mode using the prebuilt containers, these environment variables are set up by the [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/environment.py). You can find all the available environment variables setup by SageMaker Training Toolkit in the above local training logs as well.\n",
    "\n",
    "#### <span style=\"color:blue\">Step 3: Execute training<span>\n",
    "\n",
    "Run the training script using below command:\n",
    "```\n",
    "# /miniconda3/bin/python catboost_train_deploy.py\n",
    "```\n",
    "\n",
    "![](./image/training_in_docker.png)\n",
    "\n",
    "You can compare the above training job outputs and the outputs in the local training logs, they should be the same.\n",
    "\n",
    "*To exit from the running docker image, you can use ctrl + D*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5562c0e",
   "metadata": {},
   "source": [
    "## Deploying trained model in local instance\n",
    "We can also deploy the trained model and perform invocation against the local endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Deploying endpoint in local mode')\n",
    "predictor = sklearn.deploy(1, 'local', serializer=csv_serializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_test, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "predictions = predictor.predict(payload)\n",
    "print('predictions: {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cfb29",
   "metadata": {},
   "source": [
    "## Clear up resources\n",
    "Delete the endpoint deployed in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a681fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
